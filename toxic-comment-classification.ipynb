{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "toxic-comment-classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marabian/toxic-comment-classification/blob/master/toxic-comment-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIn9UllDiY27"
      },
      "source": [
        "## Toxic Comment Classification\n",
        "***\n",
        "\n",
        "[Link to Kaggle challenge/dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ayCVuciY28"
      },
      "source": [
        "import os\n",
        "import random as rnd\n",
        "\n",
        "# data analysis and wrangling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# machine learning\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3nbWyyWxSTN",
        "outputId": "272030c3-f012-4ce3-8070-68d649534797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# to mount google drive for colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzzxZRECiY3A",
        "outputId": "9c5435fe-5225-41ea-ada1-005ded5cd536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check gpu support\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4EZP9hmiY3D"
      },
      "source": [
        "# check if gpu working\n",
        "# tf.test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPY2Xk3NiY3F"
      },
      "source": [
        "# change notebook theme\n",
        "#!jt -t monokai -T -N -kl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwXP2iviY3H"
      },
      "source": [
        "## Workflow\n",
        "\n",
        "\n",
        "* Defining the problem and assembling a dataset<br>\n",
        "\n",
        "\n",
        "* Choosing a measure of success\n",
        "\n",
        "\n",
        "* Deciding on an evaluation protocol\n",
        "\n",
        "\n",
        "* Preparing your data for ML algorithms\n",
        "\n",
        "\n",
        "* Developing a model that does better than a baseline\n",
        "\n",
        "\n",
        "* Scaling up: developing a model that overfits\n",
        "\n",
        "\n",
        "* Regularizing your model and tuning your hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q595P4FZiY3M"
      },
      "source": [
        "## Defining the problem and assembling a dataset\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDDjkaX3yXZK"
      },
      "source": [
        "We are challenged to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate. Our training data will consist of ~160,000 comments from Wikipedia’s talk page edits with labeled levels of toxicity. Detecting toxic comments online will hopefully help online discussion become more productive and respectful.<br>\n",
        "\n",
        "Since we have many classes, this problem is an instance of multiclass classification; and because each data point could belong to multiple categories (in this case, levels of toxicity), this is a **multilabel, multiclass classification problem**.\n",
        "\n",
        "We will start with a solving a subset of this problem, using **binary classification** to classify between \"good\" and \"toxic\" comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU5_EFwrw-IS"
      },
      "source": [
        "path_to_data = \"/content/drive/My Drive/machine learning/notebooks/data/jigsaw-toxic-comment-classification-challenge/\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Cp997UiY3M",
        "outputId": "f5c4f001-faaa-466a-dd23-f06e34a2b8a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "train_df = pd.read_csv(path_to_data + \"train.csv\")\n",
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "train_df\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159566</th>\n",
              "      <td>ffe987279560d7ff</td>\n",
              "      <td>\":::::And for the second time of asking, when ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159567</th>\n",
              "      <td>ffea4adeee384e90</td>\n",
              "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159568</th>\n",
              "      <td>ffee36eab5c267c9</td>\n",
              "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159569</th>\n",
              "      <td>fff125370e4aaaf3</td>\n",
              "      <td>And it looks like it was actually you who put ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159570</th>\n",
              "      <td>fff46fc426af1f9a</td>\n",
              "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159571 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id  ... identity_hate\n",
              "0       0000997932d777bf  ...             0\n",
              "1       000103f0d9cfb60f  ...             0\n",
              "2       000113f07ec002fd  ...             0\n",
              "3       0001b41b1c6bb37e  ...             0\n",
              "4       0001d958c54c6e35  ...             0\n",
              "...                  ...  ...           ...\n",
              "159566  ffe987279560d7ff  ...             0\n",
              "159567  ffea4adeee384e90  ...             0\n",
              "159568  ffee36eab5c267c9  ...             0\n",
              "159569  fff125370e4aaaf3  ...             0\n",
              "159570  fff46fc426af1f9a  ...             0\n",
              "\n",
              "[159571 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo4QjJV976L4"
      },
      "source": [
        "Let's build a new dataframe containing just the training examples for our binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdF6ssXk75qq"
      },
      "source": [
        "bin_df = pd.concat([\n",
        "           train_df[train_df[labels].eq(0).all(1)][['id', 'comment_text', 'toxic']], # get new df with rows for \"good\" comments\n",
        "           train_df[train_df['toxic'] == 1][['id', 'comment_text', 'toxic']]] # get new dataframe with rows for \"toxic\" comments\n",
        "         ).reset_index(drop=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr58geauiY3P",
        "outputId": "2f1c8a97-248c-49ee-b8ad-82aa27c7af39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "bin_df"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158635</th>\n",
              "      <td>fef4cf7ba0012866</td>\n",
              "      <td>\"\\n\\n our previous conversation \\n\\nyou fuckin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158636</th>\n",
              "      <td>ff39a2895fc3b40e</td>\n",
              "      <td>YOU ARE A MISCHIEVIOUS PUBIC HAIR</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158637</th>\n",
              "      <td>ffa33d3122b599d6</td>\n",
              "      <td>Your absurd edits \\n\\nYour absurd edits on gre...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158638</th>\n",
              "      <td>ffb47123b2d82762</td>\n",
              "      <td>\"\\n\\nHey listen don't you ever!!!! Delete my e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158639</th>\n",
              "      <td>ffbdbb0483ed0841</td>\n",
              "      <td>and i'm going to keep posting the stuff u dele...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>158640 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id  ... toxic\n",
              "0       0000997932d777bf  ...     0\n",
              "1       000103f0d9cfb60f  ...     0\n",
              "2       000113f07ec002fd  ...     0\n",
              "3       0001b41b1c6bb37e  ...     0\n",
              "4       0001d958c54c6e35  ...     0\n",
              "...                  ...  ...   ...\n",
              "158635  fef4cf7ba0012866  ...     1\n",
              "158636  ff39a2895fc3b40e  ...     1\n",
              "158637  ffa33d3122b599d6  ...     1\n",
              "158638  ffb47123b2d82762  ...     1\n",
              "158639  ffbdbb0483ed0841  ...     1\n",
              "\n",
              "[158640 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GooB2kaN9O3Y"
      },
      "source": [
        "**Plot distribution of toxic comment frequency**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBnWsAje7gtB",
        "outputId": "3bd9d16d-2958-4353-9a01-503623030521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.countplot(x=\"toxic\", data=bin_df)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWhdY4fH9aJU",
        "outputId": "ba9e52b6-6207-4e2d-a61c-eeab2e20521e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(bin_df['toxic'].value_counts())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    143346\n",
            "1     15294\n",
            "Name: toxic, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXVbw51B-Kqm"
      },
      "source": [
        "As we can see, our classes are heavily imbalanced. There are a lot more \"good\" comments than \"toxic\". To deal with this class imbalance problem, we will choose ***precision and recall*** as our **evaluation metric**. This will be our measure of success."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NPPiVd-JTi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnSuKR7z-4gW"
      },
      "source": [
        "## Preparing the raw text data\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IKpS-H6_BFB"
      },
      "source": [
        "Let's start doing some text preprocessing! In NLP, the main steps to preprocess raw text into tensors includes the following steps:<br> ***word tokenization***, ***text cleaning*** (lowercasing, removing stop words and punctuation, stemming), and ***text vectorization***.\n",
        "\n",
        "Our first *vectorization scheme* will be using **TF-IDF vectors** for the  most common words found in our corpus.\n",
        "\n",
        "In information retrieval, **TF-IDF**, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. We will use *Scikit-Learn's* awesome **TfidfVectorizer** to convert a collection of raw documents to a matrix of TF-IDF features. This does all the text preprocessing steps we need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIArd-inDEQo"
      },
      "source": [
        "### Bulding feature vectors using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHfgx0c-5W6",
        "outputId": "150efcfa-1977-43f1-f7fc-2367e8b83a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(lowercase='True', stop_words='english', max_features=10)\n",
        "X = vectorizer.fit_transform(bin_df.comment_text)\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['actually', 'add', 'added', 'article', 'articles', 'ask', 'believe', 'best', 'better', 'block', 'blocked', 'case', 'change', 'com', 'comment', 'comments', 'content', 'deleted', 'deletion', 'did', 'didn', 'discussion', 'does', 'doesn', 'don', 'edit', 'editing', 'editors', 'edits', 'fact', 'feel', 'free', 'fuck', 'going', 'good', 'help', 'hi', 'history', 'hope', 'http', 'image', 'information', 'just', 'know', 'let', 'like', 'link', 'links', 'list', 'll', 'look', 'make', 'need', 'new', 'note', 'page', 'pages', 'people', 'person', 'personal', 'place', 'point', 'policy', 'question', 'questions', 'read', 'really', 'reason', 'removed', 'right', 'said', 'say', 'section', 'source', 'sources', 'stop', 'subject', 'sure', 'talk', 'thank', 'thanks', 'thing', 'things', 'think', 'time', 'use', 'used', 'user', 'using', 'utc', 'vandalism', 've', 'want', 'way', 'welcome', 'wiki', 'wikipedia', 'work', 'wp', 'wrong']\n",
            "(158640, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2YwOa0HEYXB"
      },
      "source": [
        "In order to get a better idea on how many words to consider as features, let's visualize the distribution of word counts amongst our training examples. In order to do this, we must first preprocess the raw text data ourselves. Let's define a function to preprocess the comments by *tokenizing* and *cleaning*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqhwYb8BTBEB",
        "outputId": "302ab772-3a17-4932-a3b2-ea0791709fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# stop words\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords_english = stopwords.words('english')"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFXJwiSoFZgq"
      },
      "source": [
        "def process_comment(comment):\n",
        "    \"\"\"Process comment function.\n",
        "    Input:\n",
        "        comment: a string containing a comment\n",
        "    Output:\n",
        "        comment_clean: a list of cleaned word tokens\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # normalizing case\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # remove special characters\n",
        "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
        "    comment = re.sub(pat, '', comment)\n",
        "\n",
        "    # remove numbers\n",
        "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n",
        "    comment = re.sub(pattern, '', comment)\n",
        "\n",
        "    # remove punctuation\n",
        "    comment = comment.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # replace repeated character sequences of length 3 or greater with sequences of length 3\n",
        "    comment = nltk.tokenize.casual.reduce_lengthening(comment)\n",
        "    \n",
        "    # tokenize comment\n",
        "    tokens = word_tokenize(comment)\n",
        "    \n",
        "    # remove stop words and stem\n",
        "    #comment_clean = [stemmer.stem(token) for token in tokens if token not in stopwords_english]\n",
        "    comment_clean = [token for token in tokens if token not in stopwords_english]\n",
        "\n",
        "    return tokens"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeUo_A5BGar9",
        "outputId": "329084ee-b522-4b99-fe28-bb383097afb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=process_comment, max_features=10)\n",
        "X = vectorizer.fit_transform(bin_df.comment_text)\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-95654ad3f3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_comment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-46f8864fea63>\u001b[0m in \u001b[0;36mprocess_comment\u001b[0;34m(comment)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# tokenize comment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# remove stop words and stem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1orkzRqNT8K",
        "outputId": "69d0acc1-12f5-4b3c-e836-e859e53e5812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aaammm', 'doesnt', 'decor', 'happi']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGAU9bRiPyXH",
        "outputId": "d96ec49e-0564-4271-dbd7-506c69c31d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nltk.tokenize.casual.reduce_lengthening(\"I AAAAAAAAAAM 23 doesn't decoration\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I AAAM 23 doesn't decoration\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9yNj9x2B9gy"
      },
      "source": [
        ""
      ]
    }
  ]
}